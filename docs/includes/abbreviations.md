<!-- Pipeline Stages -->
*[Stage A]: Builder: reads YAML configs, constructs EpiModel objects for each population, and packages them into task inputs
*[Stage B]: Runner: executes simulations or calibrations in parallel, one per task input
*[Stage C]: Output Generator: aggregates all results into formatted CSVs, plots, and metadata

<!-- Core Modeling -->
*[EpiModel]: Compartmental epidemic model object (S-I-R, etc.) from the epydemix engine
*[ABCSampler]: Approximate Bayesian Computation sampler from epydemix. Wraps an EpiModel with priors, observed data, and calibration logic
*[epydemix]: Core Python package for compartmental epidemic modeling
<!-- *[epymodelingsuite]: Python package that wraps epydemix with configuration, builders, calibration, and dispatching -->
*[dill]: Python serialization library (like pickle) that handles lambda functions and closures

<!-- Dispatcher Schema (epymodelingsuite.schema.dispatcher) -->
*[BuilderOutput]: Stage A output. Bundles a model (EpiModel or ABCSampler) with execution instructions (simulation, calibration, or projection settings)
*[SimulationOutput]: Stage B output (simulation). Wraps SimulationResults with task metadata (population, seed, timestep)
*[CalibrationOutput]: Stage B output (calibration). Wraps CalibrationResults with task metadata (population, seed, timestep)
*[SimulationResults]: epydemix dataclass containing simulation trajectories, parameters, and methods for computing quantiles
*[CalibrationResults]: epydemix dataclass containing posterior distributions, selected trajectories, projections, and calibration diagnostics

<!-- Pipeline Concepts -->
*[EXP_ID]: Experiment identifier, a name grouping related runs (e.g., flu-forecast-2024-w42)
*[RUN_ID]: Unique run identifier, auto-generated as YYYYMMDD-HHMMSS-uuid
*[DIR_PREFIX]: Base directory prefix for storage paths (default: pipeline/flu/)
*[NUM_TASKS]: Number of parallel tasks, set by Stage A, used by Stage B and C
<!-- *[builder artifacts]: Pickled task input files (.pkl) generated by Stage A -->
<!-- *[runner artifacts]: Pickled result files (.pkl) generated by Stage B -->

<!-- General -->
*[PAT]: Personal Access Token, a GitHub token used to authenticate with private repositories
*[IAM]: Identity and Access Management: system for managing permissions and access control
*[VM]: Virtual Machine: a compute instance provisioned by Cloud Batch to run containers

<!-- Google Cloud -->
*[GCS]: Google Cloud Storage: object storage for pipeline artifacts and results
*[Cloud Batch]: Google Cloud serverless compute service that provisions VMs and runs containers
*[Cloud Workflows]: Google Cloud orchestration service that coordinates Stage A → B → C
*[Artifact Registry]: Google Cloud Docker image repository
*[Cloud Storage]: Google Cloud Storage (GCS): object storage for pipeline artifacts and results
*[Cloud Logging]: Google Cloud centralized log aggregation service
*[Cloud Monitoring]: Google Cloud service for tracking metrics, alerting on failures, and budget thresholds
*[Cloud Build]: Google Cloud service for building Docker images remotely

<!-- Execution -->
*[BATCH_TASK_INDEX]: Environment variable automatically set by Cloud Batch (0-based task index)
*[TASK_INDEX]: Environment variable manually set for local execution (overrides BATCH_TASK_INDEX)
*[EXECUTION_MODE]: Environment variable controlling storage backend: "cloud" (GCS) or "local" (filesystem)

<!-- Tools -->
*[epycloud]: CLI tool for managing the epymodelingsuite-cloud pipeline
*[YAML]: Human-readable data format used for configuration files
*[Terraform]: Infrastructure as Code tool used to deploy Google Cloud resources
*[Docker Compose]: Tool for running multi-container Docker applications locally
