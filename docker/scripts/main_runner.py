#!/usr/bin/env python3
"""
Stage B: Process individual tasks using BATCH_TASK_INDEX or TASK_INDEX.
Loads pickled input from storage (GCS or local), runs simulation, saves results.
"""

import os
import sys

import dill  # Use dill instead of pickle for better serialization support

# Add scripts directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from epymodelingsuite.dispatcher import dispatch_runner
from epymodelingsuite.telemetry import ExecutionTelemetry
from util import storage
from util.error_handling import handle_stage_error
from util.logger import setup_logger

# Task index formatting (supports up to 99999 tasks)
INDEX_WIDTH = 5


def main() -> None:
    """
    Stage B (Runner): Process individual task in parallel processing pipeline.

    This stage:
    1. Retrieves task index from environment (TASK_INDEX or BATCH_TASK_INDEX)
    2. Loads pickled input file generated by Stage A
    3. Calls epymodelingsuite.dispatcher.dispatch_runner() to execute workload
    4. Saves pickled result file to storage
    5. Saves telemetry summary for this task

    Environment Variables
    ---------------------
    TASK_INDEX : str (optional)
        Task index for local execution (0-based)
    BATCH_TASK_INDEX : str (optional)
        Task index for cloud execution (0-based, auto-set by Cloud Batch)
    EXP_ID : str (required)
        Experiment identifier
    RUN_ID : str (optional)
        Run identifier (defaults to "unknown")
    EXECUTION_MODE : str
        Storage mode: "cloud" (GCS) or "local" (filesystem)
    DIR_PREFIX : str (optional)
        Base directory prefix (default: "pipeline/flu/")
    LOG_LEVEL : str (optional)
        Logging level: DEBUG, INFO, WARNING, ERROR (default: INFO)

    Inputs
    ------
    Reads from storage:
        - builder-artifacts/input_{task_index:05d}.pkl : Pickled workload input

    Outputs
    -------
    Saves to storage:
        - runner-artifacts/result_{task_index:05d}.pkl : Pickled result
        - summaries/json/runner_{task_index:05d}_summary.json : Telemetry metadata
        - summaries/txt/runner_{task_index:05d}_summary.txt : Human-readable telemetry

    Raises
    ------
    FileNotFoundError
        If input file doesn't exist
    ValueError
        If input file cannot be deserialized
    SystemExit
        Exits with code 1 on any fatal error
    """
    try:
        # Get task index - supports both local (TASK_INDEX) and cloud (BATCH_TASK_INDEX)
        idx = int(os.getenv("TASK_INDEX", os.environ.get("BATCH_TASK_INDEX", "0")))

        # Get configuration
        config = storage.get_config()

        # Setup logger with task context
        logger = setup_logger(
            "runner",
            task_index=idx,
            exp_id=config["exp_id"],
            run_id=config["run_id"],
        )

        logger.info("Starting task")
        logger.info(
            "Storage configuration",
            extra={
                "mode": config["mode"],
                "dir_prefix": config["dir_prefix"],
                "bucket": config.get("bucket", "N/A"),
            },
        )

        # Load input file (workload from dispatcher)
        input_path = storage.get_path("builder-artifacts", f"input_{idx:0{INDEX_WIDTH}d}.pkl.gz")
        logger.debug(f"Loading input: {input_path}")

        try:
            raw_data = storage.load_bytes(input_path)
            workload = dill.loads(raw_data)
            logger.debug(f"Input loaded: {len(raw_data):,} bytes")
        except Exception as e:
            logger.error(f"Failed to load input: {e}")
            raise

        # Wrap runner in telemetry context
        with ExecutionTelemetry() as runner_telemetry:
            # Run simulation/calibration using dispatch_runner
            try:
                result = dispatch_runner(workload)
            except Exception as e:
                logger.error(f"Run failed: {e}")
                raise

            # Save results
            output_path = storage.get_path("runner-artifacts", f"result_{idx:0{INDEX_WIDTH}d}.pkl.gz")
            logger.debug(f"Saving results: {output_path}")

            try:
                output_data = dill.dumps(result)
                storage.save_bytes(output_path, output_data)
                logger.debug(f"Results saved: {len(output_data):,} bytes")
            except Exception as e:
                logger.error(f"Failed to save results: {e}")
                raise

            # Save telemetry as JSON and TXT (will be aggregated in Stage C)
            storage.save_telemetry_summary(
                runner_telemetry, f"runner_{idx:0{INDEX_WIDTH}d}_summary"
            )

        logger.info("Task complete")

    except Exception as e:
        handle_stage_error(f"Task {idx}", e, logger)


if __name__ == "__main__":
    main()
