# Environment variables for the epymodelingsuite-cloud pipeline
# Copy this to .env and fill in your values
# Source with: source .env

# Google Cloud Infrastructure
export PROJECT_ID=your-gcp-project-id
export REGION=us-central1
export REPO_NAME=epymodelingsuite-repo
export BUCKET_NAME=your-bucket-name

# Docker image configuration
export IMAGE_NAME=epymodelingsuite
export IMAGE_TAG=latest

# GitHub Private Repositories
# Forecast data repository (format: owner/repo)
export GITHUB_FORECAST_REPO=owner/forecasting-repo

# Modeling suite package repository (format: owner/repo)
export GITHUB_MODELING_SUITE_REPO=owner/modeling-suite-repo
# Branch or commit to build from
export GITHUB_MODELING_SUITE_REF=main

# Workflow run parameters
export DIR_PREFIX=pipeline/flu/
export MAX_PARALLELISM=100
export TASK_COUNT_PER_NODE=1  # Max tasks per VM (1 = dedicated VM per task, 2+ = shared VMs)

# Batch machine configuration - Stage A (Dispatcher)
export STAGE_A_CPU_MILLI=2000
export STAGE_A_MEMORY_MIB=4096
export STAGE_A_MACHINE_TYPE=""  # Optional: e2-standard-2, n2-standard-4, etc. Empty = auto-select
export STAGE_A_MAX_RUN_DURATION=3600  # Maximum task duration in seconds (3600s = 1 hour)

# Batch machine configuration - Stage B (Runner)
export STAGE_B_CPU_MILLI=2000
export STAGE_B_MEMORY_MIB=4096
export STAGE_B_MACHINE_TYPE=""  # Optional: e2-standard-2, c4d-standard-2, etc. Empty = auto-select
export STAGE_B_MAX_RUN_DURATION=36000  # Maximum task duration in seconds (36000s = 10 hours)
# Note: If STAGE_B_MACHINE_TYPE is set, cpuMilli/memoryMib are constraints (task requirements). If empty, Google Cloud selects VM based on cpuMilli/memoryMib.

# Batch machine configuration - Stage C (Output)
export STAGE_C_CPU_MILLI=2000
export STAGE_C_MEMORY_MIB=8192
export STAGE_C_MACHINE_TYPE=""  # Optional: e2-standard-2, n2-standard-4, etc. Empty = auto-select
export STAGE_C_MAX_RUN_DURATION=7200  # Maximum task duration in seconds (7200s = 2 hours)
export RUN_OUTPUT_STAGE=true  # Set to false to skip Stage C (Output generation)

# EXP_ID is required when running the pipeline.
# EXP_ID=my-experiment-id make run-workflow

# Logging configuration
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL (default: INFO)
export LOG_LEVEL=INFO

# Storage operation verbosity (logs every read/write operation)
# Set to "true" to see detailed I/O operations (default: true)
# With dedicated VMs per task (TASK_COUNT_PER_NODE=1), verbose logging is not an issue
export STORAGE_VERBOSE=true

# Note: GitHub PAT should be stored in Secret Manager (for cloud) or in .env.local
# DO NOT store it in this file
# Create secret: echo -n "your_pat" | gcloud secrets create github-pat --data-file=-
