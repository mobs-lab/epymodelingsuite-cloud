"""
Storage abstraction layer for switching between GCS and local filesystem.

Supports two execution modes via EXECUTION_MODE environment variable:
- 'cloud': Uses Google Cloud Storage API
- 'local': Uses filesystem operations at /data/bucket/ mount

The storage layer automatically constructs paths based on configuration:
- In local mode: /data/bucket/{path}
- In cloud mode: gs://{GCS_BUCKET}/{path}

Usage:
    from util.storage import load_bytes, save_bytes, get_path

    # Build a path
    input_path = get_path("builder-artifacts", "input_0000.pkl")

    # Load/save works in both modes
    data = load_bytes(input_path)
    save_bytes(output_path, data)
"""

import os
from pathlib import Path
from typing import Optional


def _get_execution_mode() -> str:
    """Get the execution mode from environment variable."""
    return os.getenv("EXECUTION_MODE", "cloud").lower()


def _get_local_base_path() -> Path:
    """Get the base path for local storage."""
    return Path(os.getenv("LOCAL_DATA_PATH", "/data"))


def get_config() -> dict:
    """
    Get storage configuration from environment variables.

    Returns:
        Dictionary with configuration:
        - mode: "local" or "cloud"
        - bucket: GCS bucket name (cloud mode only)
        - exp_id: Experiment ID
        - run_id: Run ID
        - dir_prefix: Directory prefix (e.g., "pipeline/flu/")

    Raises:
        ValueError: If EXP_ID is not set in environment
    """
    exp_id = os.getenv("EXP_ID")
    if not exp_id:
        raise ValueError(
            "EXP_ID environment variable is required but not set. "
            "Set it before running: export EXP_ID=your-experiment-id"
        )

    return {
        "mode": _get_execution_mode(),
        "bucket": os.getenv("GCS_BUCKET", ""),
        "exp_id": exp_id,
        "run_id": os.getenv("RUN_ID", "unknown"),
        "dir_prefix": os.getenv("DIR_PREFIX", "pipeline/flu/").rstrip("/"),
    }


def get_path(*parts: str) -> str:
    """
    Construct a storage path from components.

    Uses DIR_PREFIX from environment (e.g., "pipeline/flu/") plus EXP_ID and RUN_ID.

    In local mode: Returns path like "bucket/{dir_prefix}/{exp_id}/{run_id}/{parts}"
    In cloud mode: Returns path like "{dir_prefix}/{exp_id}/{run_id}/{parts}"

    Args:
        *parts: Path components to join (e.g., "builder-artifacts", "input_0000.pkl")

    Returns:
        Full storage path string

    Example:
        # With DIR_PREFIX=pipeline/flu/, EXP_ID=test-sim, RUN_ID=run-20241015
        get_path("builder-artifacts", "input_0000.pkl")
        # Local: "bucket/pipeline/flu/test-sim/run-20241015/builder-artifacts/input_0000.pkl"
        # Cloud: "pipeline/flu/test-sim/run-20241015/builder-artifacts/input_0000.pkl"
    """
    config = get_config()
    mode = config["mode"]

    # Build the base path structure using DIR_PREFIX
    base_parts = [config["dir_prefix"], config["exp_id"], config["run_id"]]
    full_parts = base_parts + list(parts)

    if mode == "local":
        # Prefix with "bucket/" for local mode
        return "bucket/" + "/".join(full_parts)
    else:
        # No bucket prefix for cloud mode
        return "/".join(full_parts)


def _resolve_storage_location(path: str) -> tuple[Optional[str], str]:
    """
    Resolve the storage location from a path.

    In local mode:
        - Returns (None, path) - path is used as filesystem path

    In cloud mode:
        - Gets bucket from GCS_BUCKET env var
        - Strips "bucket/" prefix if present
        - Returns (bucket_name, clean_path)

    Args:
        path: Storage path (may have "bucket/" prefix from local mode)

    Returns:
        Tuple of (bucket_name, final_path)
    """
    config = get_config()
    mode = config["mode"]

    if mode == "local":
        # Local mode: use path as-is
        return None, path
    else:
        # Cloud mode: get bucket from env and clean path
        bucket = config["bucket"]
        if not bucket:
            raise ValueError(
                "Cloud mode requires GCS_BUCKET environment variable to be set"
            )

        # Strip "bucket/" prefix if present (for paths generated in local mode)
        clean_path = path
        if path.startswith("bucket/"):
            clean_path = "/".join(path.split("/")[1:])

        return bucket, clean_path


def load_bytes(path: str) -> bytes:
    """
    Load bytes from storage.

    In cloud mode: Downloads from GCS bucket (using GCS_BUCKET env var)
    In local mode: Reads from local filesystem at /data/{path}

    Args:
        path: Storage path (generated by get_path() or custom)

    Returns:
        File contents as bytes

    Raises:
        FileNotFoundError: If file doesn't exist
        ValueError: In cloud mode if GCS_BUCKET not set
        Exception: For GCS errors in cloud mode
    """
    mode = _get_execution_mode()
    bucket_name, final_path = _resolve_storage_location(path)

    if mode == "local":
        # Local filesystem mode
        base_path = _get_local_base_path()
        file_path = base_path / final_path

        if not file_path.exists():
            raise FileNotFoundError(f"Local file not found: {file_path}")

        print(f"[Local Storage] Reading: {file_path}")
        return file_path.read_bytes()

    else:
        # Cloud mode - use GCS
        from google.cloud import storage

        client = storage.Client()
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(final_path)

        print(f"[GCS] Downloading: gs://{bucket_name}/{final_path}")
        return blob.download_as_bytes()


def save_bytes(path: str, data: bytes) -> None:
    """
    Save bytes to storage.

    In cloud mode: Uploads to GCS bucket (using GCS_BUCKET env var)
    In local mode: Writes to local filesystem at /data/{path}

    Args:
        path: Storage path (generated by get_path() or custom)
        data: File contents as bytes

    Raises:
        ValueError: In cloud mode if GCS_BUCKET not set
        Exception: For I/O or GCS errors
    """
    mode = _get_execution_mode()
    bucket_name, final_path = _resolve_storage_location(path)

    if mode == "local":
        # Local filesystem mode
        base_path = _get_local_base_path()
        file_path = base_path / final_path

        # Create parent directories if they don't exist
        file_path.parent.mkdir(parents=True, exist_ok=True)

        # Handle both bytes and str (in case to_csv returns str instead of bytes)
        if isinstance(data, str):
            data = data.encode('utf-8')

        print(f"[Local Storage] Writing: {file_path} ({len(data)} bytes)")
        file_path.write_bytes(data)

    else:
        # Cloud mode - use GCS
        from google.cloud import storage

        client = storage.Client()
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(final_path)

        # Handle both bytes and str (in case to_csv returns str instead of bytes)
        if isinstance(data, str):
            data = data.encode('utf-8')

        print(f"[GCS] Uploading: gs://{bucket_name}/{final_path} ({len(data)} bytes)")
        blob.upload_from_string(data)


def list_blobs(bucket_name: Optional[str], prefix: str = "") -> list[str]:
    """
    List all blob paths in storage with given prefix.

    In cloud mode: Lists blobs in GCS bucket
    In local mode: Lists files in local filesystem

    Args:
        bucket_name: GCS bucket name (required in cloud mode, can be None in local mode)
        prefix: Path prefix to filter results

    Returns:
        List of blob/file paths
    """
    mode = _get_execution_mode()

    if mode == "local":
        # Local filesystem mode
        base_path = _get_local_base_path()
        search_path = base_path / prefix if prefix else base_path

        if not search_path.exists():
            return []

        # Find all files recursively under the prefix path
        files = []
        if search_path.is_file():
            files = [str(search_path.relative_to(base_path))]
        else:
            for file_path in search_path.rglob("*"):
                if file_path.is_file():
                    files.append(str(file_path.relative_to(base_path)))

        return sorted(files)

    else:
        # Cloud mode - use GCS
        from google.cloud import storage

        client = storage.Client()
        bucket = client.bucket(bucket_name)
        blobs = bucket.list_blobs(prefix=prefix)

        return sorted([blob.name for blob in blobs])


def get_mode_info() -> dict[str, str]:
    """
    Get information about the current storage mode.

    Returns:
        Dictionary with mode information
    """
    mode = _get_execution_mode()
    info = {"mode": mode}

    if mode == "local":
        info["base_path"] = str(_get_local_base_path())

    return info
