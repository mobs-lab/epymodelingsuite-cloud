# Epymodelingsuite Pipeline Workflow

main:
  params: [input]
  steps:
    # ========== INITIALIZATION ==========
    - initialize_workflow:
        assign:
          # Get environment info
          - project: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
          - location: $${sys.get_env("GOOGLE_CLOUD_LOCATION")}
          # Allow runtime override of image tag (defaults to terraform variable if not provided)
          - imageTag: $${default(map.get(input, "imageTag"), "${image_tag}")}
          - repoUri: $${location + "-docker.pkg.dev/" + project + "/${repo_name}/${image_name}:" + imageTag}
          - githubForecastRepo: $${default(map.get(input, "githubForecastRepo"), "")}
          - forecastRepoRef: $${default(map.get(input, "forecastRepoRef"), "")}
          - outputConfigFile: $${default(map.get(input, "outputConfigFile"), "")}

          # Generate unique run ID with datetime prefix from execution ID
          - executionId: $${sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID")}
          - executionIdShort: $${text.split(executionId, "/")[len(text.split(executionId, "/")) - 1]}
          - nowTimestamp: $${sys.now()}
          # Format timestamp as ISO 8601: YYYY-MM-DDTHH:MM:SS.sssZ
          - timestamp: $${time.format(nowTimestamp)}
          # Extract date part (YYYYMMDD) from timestamp like "2025-10-16T14:30:52.123Z"
          - datePart: $${text.substring(text.replace_all(timestamp, "-", ""), 0, 8)}
          # Extract time part (HHMMSS) from timestamp
          - timePart: $${text.substring(text.replace_all(text.substring(timestamp, 11, 19), ":", ""), 0, 6)}
          # Take first 8 characters of UUID for uniqueness
          - uniqueId: $${text.substring(executionIdShort, 0, 8)}
          # Combine: YYYYMMDD-HHMMSS-<uuid-prefix>
          - runId: $${datePart + "-" + timePart + "-" + uniqueId}

          # Sanitize exp_id for use in labels (replace / with - for GCP label compliance)
          - expIdLabel: $${text.replace_all(input.exp_id, "/", "-")}

          # Build GCS paths
          - dirPrefixRaw: $${default(map.get(input, "dirPrefix"), "")}
          - dirPrefix: $${if(dirPrefixRaw == "", "", if(text.substring(dirPrefixRaw, len(dirPrefixRaw) - 1, len(dirPrefixRaw)) == "/", dirPrefixRaw, dirPrefixRaw + "/"))}
          - expId: $${input.exp_id}
          - bucket: $${input.bucket}
          - basePath: $${dirPrefix + expId + "/" + runId + "/"}
          - outPrefix: $${basePath + "builder-artifacts/"}
          - inPrefix: $${basePath + "builder-artifacts/"}
          - resPrefix: $${basePath + "runner-artifacts/"}

          # Job parameters
          - batchSaEmail: $${input.batchSaEmail}
          - maxParallelism: $${default(map.get(input, "maxParallelism"), 100)}
          - runOutputStage: ${run_output_stage}

          # Network configuration
          - networkSelfLink: "${network_self_link}"
          - subnetName: "${subnet_name}"
          - subnetSelfLink: "${subnet_self_link}"

    # Stage resource configuration (split to stay under 50 variable limit)
    - initialize_stage_resources:
        assign:
          # Stage A - allow runtime override of machine type and resources
          - machineTypeAOverride: $${default(map.get(input, "stageAMachineType"), "")}
          - cpuMilliAOverride: $${default(map.get(input, "stageACpuMilli"), 0)}
          - memoryMibAOverride: $${default(map.get(input, "stageAMemoryMib"), 0)}
          - cpuMilliA: $${if(cpuMilliAOverride > 0, cpuMilliAOverride, ${stage_a_cpu_milli})}
          - memoryMibA: $${if(memoryMibAOverride > 0, memoryMibAOverride, ${stage_a_memory_mib})}
          - machineTypeA: $${if(machineTypeAOverride != "", machineTypeAOverride, "${stage_a_machine_type}")}
          - maxRunDurationA: "${stage_a_max_run_duration}s"
          # Stage B - allow runtime override of machine type and resources
          - machineTypeBOverride: $${default(map.get(input, "stageBMachineType"), "")}
          - cpuMilliBOverride: $${default(map.get(input, "stageBCpuMilli"), 0)}
          - memoryMibBOverride: $${default(map.get(input, "stageBMemoryMib"), 0)}
          - cpuMilliB: $${if(cpuMilliBOverride > 0, cpuMilliBOverride, ${stage_b_cpu_milli})}
          - memoryMibB: $${if(memoryMibBOverride > 0, memoryMibBOverride, ${stage_b_memory_mib})}
          - machineTypeB: $${if(machineTypeBOverride != "", machineTypeBOverride, "${stage_b_machine_type}")}
          - maxRunDurationB: "${stage_b_max_run_duration}s"
          # Stage C - allow runtime override of machine type and resources
          - machineTypeCOverride: $${default(map.get(input, "stageCMachineType"), "")}
          - cpuMilliCOverride: $${default(map.get(input, "stageCCpuMilli"), 0)}
          - memoryMibCOverride: $${default(map.get(input, "stageCMemoryMib"), 0)}
          - cpuMilliC: $${if(cpuMilliCOverride > 0, cpuMilliCOverride, ${stage_c_cpu_milli})}
          - memoryMibC: $${if(memoryMibCOverride > 0, memoryMibCOverride, ${stage_c_memory_mib})}
          - machineTypeC: $${if(machineTypeCOverride != "", machineTypeCOverride, "${stage_c_machine_type}")}
          - maxRunDurationC: "${stage_c_max_run_duration}s"
          # Task count per node - allow runtime override
          - taskCountPerNodeOverride: $${default(map.get(input, "taskCountPerNode"), 0)}
          - taskCountPerNode: $${if(taskCountPerNodeOverride > 0, taskCountPerNodeOverride, ${task_count_per_node})}

    # ========== STAGE A: Generate Input Files ==========
    - create_stageA_job:
        call: http.post
        args:
          url: $${"https://batch.googleapis.com/v1/projects/" + project + "/locations/" + location + "/jobs"}
          auth:
            type: OAuth2
            scopes: https://www.googleapis.com/auth/cloud-platform
          body:
            labels:
              component: epymodelingsuite
              project: epymodelingsuite-cloud
              environment: production
              stage: builder
              exp_id: $${expIdLabel}
              run_id: $${runId}
              managed-by: workflows
            taskGroups:
              - taskCount: 1
                taskSpec:
                  runnables:
                    - container:
                        imageUri: $${repoUri}
                        entrypoint: "/bin/bash"
                        commands:
                          - "/scripts/run_builder.sh"
                  environment:
                    variables:
                      EXECUTION_MODE: "cloud"
                      GCS_BUCKET: $${bucket}
                      DIR_PREFIX: $${dirPrefix}
                      EXP_ID: $${expId}
                      RUN_ID: $${runId}
                      GITHUB_FORECAST_REPO: $${githubForecastRepo}
                      FORECAST_REPO_REF: $${forecastRepoRef}
                      GCLOUD_PROJECT_ID: $${project}
                      GITHUB_PAT_SECRET: "github-pat"
                      FORECAST_REPO_DIR: "/data/forecast/"
                  computeResource:
                    cpuMilli: $${cpuMilliA}
                    memoryMib: $${memoryMibA}
                  maxRunDuration: $${maxRunDurationA}
            logsPolicy:
              destination: "CLOUD_LOGGING"
            allocationPolicy:
              location:
                allowedLocations:
                  - $${"regions/" + location}
              serviceAccount:
                email: $${batchSaEmail}
              network:
                networkInterfaces:
                  - network: $${networkSelfLink}
                    subnetwork: $${subnetSelfLink}
                    noExternalIpAddress: true
              %{~ if stage_a_machine_type != "" ~}
              instances:
                - %{~ if length(regexall("^c4d-", stage_a_machine_type)) > 0 ~}
                  # C4D machine types require Hyperdisk (hyperdisk-balanced or hyperdisk-extreme)
                  # Disk cannot be smaller than the chosen image (30GB)
                  installGpuDrivers: false
                  policy:
                    machineType: $${machineTypeA}
                    provisioningModel: STANDARD
                    bootDisk:
                      type: hyperdisk-balanced
                      sizeGb: 30
                  %{~ else ~}
                  policy:
                    machineType: $${machineTypeA}
                  %{~ endif ~}
              %{~ else ~}
              instances:
                - policy: {}
              %{~ endif ~}
              labels:
                component: epymodelingsuite
                project: epymodelingsuite-cloud
                environment: production
                stage: builder
                managed-by: workflows
          query:
            jobId: $${"stage-a-" + uniqueId}
        result: jobA

    - wait_for_stageA_creation:
        call: sys.sleep
        args:
          seconds: 5

    - wait_for_stageA_completion:
        call: waitJob
        args:
          name: $${jobA.body.name}

    - wait_for_file_upload:
        call: sys.sleep
        args:
          seconds: 15

    - wait_for_input_files:
        call: waitForFiles
        args:
          bucket: $${bucket}
          prefix: $${outPrefix}
          maxRetries: 60
          sleepSeconds: 5
        result: inputFiles

    - count_input_files:
        assign:
          - items: $${default(inputFiles, [])}
          - N: $${len(items)}

    - validate_inputs:
        switch:
          - condition: $${N <= 0}
            raise: "No inputs produced by Stage A"

    # ========== STAGE B: Run Simulations ==========
    - calculate_parallelism:
        assign:
          - parallelism: $${if(N > maxParallelism, maxParallelism, N)}

    - create_stageB_job:
        call: http.post
        args:
          url: $${"https://batch.googleapis.com/v1/projects/" + project + "/locations/" + location + "/jobs"}
          auth:
            type: OAuth2
          body:
            labels:
              component: epymodelingsuite
              project: epymodelingsuite-cloud
              environment: production
              stage: runner
              exp_id: $${expIdLabel}
              run_id: $${runId}
              managed-by: workflows
            taskGroups:
              - taskCount: $${N}
                parallelism: $${parallelism}
                taskCountPerNode: $${taskCountPerNode}
                taskSpec:
                  runnables:
                    - container:
                        imageUri: $${repoUri}
                        entrypoint: "/bin/bash"
                        commands:
                          - "/scripts/run_runner.sh"
                  environment:
                    variables:
                      EXECUTION_MODE: "cloud"
                      GCS_BUCKET: $${bucket}
                      DIR_PREFIX: $${dirPrefix}
                      EXP_ID: $${expId}
                      RUN_ID: $${runId}
                      GITHUB_FORECAST_REPO: $${githubForecastRepo}
                      FORECAST_REPO_REF: $${forecastRepoRef}
                      GCLOUD_PROJECT_ID: $${project}
                      GITHUB_PAT_SECRET: "github-pat"
                      FORECAST_REPO_DIR: "/data/forecast/"
                  computeResource:
                    cpuMilli: $${cpuMilliB}
                    memoryMib: $${memoryMibB}
                  maxRunDuration: $${maxRunDurationB}
            logsPolicy:
              destination: "CLOUD_LOGGING"
            allocationPolicy:
              location:
                allowedLocations:
                  - $${"regions/" + location}
              serviceAccount:
                email: $${batchSaEmail}
              network:
                networkInterfaces:
                  - network: $${networkSelfLink}
                    subnetwork: $${subnetSelfLink}
                    noExternalIpAddress: true
              %{~ if stage_b_machine_type != "" ~}
              instances:
                - %{~ if length(regexall("^c4d-", stage_b_machine_type)) > 0 ~}
                  # C4D machine types require Hyperdisk (hyperdisk-balanced or hyperdisk-extreme)
                  # C4D does NOT support regular Persistent Disks (pd-ssd, pd-balanced, etc.)
                  installGpuDrivers: false
                  policy:
                    machineType: $${machineTypeB}
                    provisioningModel: STANDARD
                    bootDisk:
                      type: hyperdisk-balanced
                      sizeGb: 50
                  %{~ else ~}
                  policy:
                    machineType: $${machineTypeB}
                  %{~ endif ~}
              %{~ else ~}
              instances:
                - policy: {}
              %{~ endif ~}
              labels:
                component: epymodelingsuite
                project: epymodelingsuite-cloud
                environment: production
                stage: runner
                managed-by: workflows
          query:
            jobId: $${"stage-b-" + uniqueId}
        result: jobB

    - wait_for_stageB_creation:
        call: sys.sleep
        args:
          seconds: 5

    - wait_for_stageB_completion:
        call: waitJob
        args:
          name: $${jobB.body.name}

    # ========== STAGE C: Generate Outputs (Conditional) ==========
    - check_run_output_stage:
        switch:
          - condition: $${runOutputStage == true}
            steps:
              - create_stageC_job:
                  call: http.post
                  args:
                    url: $${"https://batch.googleapis.com/v1/projects/" + project + "/locations/" + location + "/jobs"}
                    auth:
                      type: OAuth2
                      scopes: https://www.googleapis.com/auth/cloud-platform
                    body:
                      labels:
                        component: epymodelingsuite
                        project: epymodelingsuite-cloud
                        environment: production
                        stage: output
                        exp_id: $${expIdLabel}
                        run_id: $${runId}
                        managed-by: workflows
                      taskGroups:
                        - taskCount: 1
                          taskSpec:
                            runnables:
                              - container:
                                  imageUri: $${repoUri}
                                  entrypoint: "/bin/bash"
                                  commands:
                                    - "/scripts/run_output.sh"
                            environment:
                              variables:
                                EXECUTION_MODE: "cloud"
                                GCS_BUCKET: $${bucket}
                                DIR_PREFIX: $${dirPrefix}
                                EXP_ID: $${expId}
                                RUN_ID: $${runId}
                                NUM_TASKS: $${string(N)}
                                GITHUB_FORECAST_REPO: $${githubForecastRepo}
                                FORECAST_REPO_REF: $${forecastRepoRef}
                                GCLOUD_PROJECT_ID: $${project}
                                GITHUB_PAT_SECRET: "github-pat"
                                FORECAST_REPO_DIR: "/data/forecast/"
                                OUTPUT_CONFIG_FILE: $${outputConfigFile}
                            computeResource:
                              cpuMilli: $${cpuMilliC}
                              memoryMib: $${memoryMibC}
                            maxRunDuration: $${maxRunDurationC}
                      logsPolicy:
                        destination: "CLOUD_LOGGING"
                      allocationPolicy:
                        location:
                          allowedLocations:
                            - $${"regions/" + location}
                        serviceAccount:
                          email: $${batchSaEmail}
                        network:
                          networkInterfaces:
                            - network: $${networkSelfLink}
                              subnetwork: $${subnetSelfLink}
                              noExternalIpAddress: true
                        %{~ if stage_c_machine_type != "" ~}
                        instances:
                          - %{~ if length(regexall("^c4d-", stage_c_machine_type)) > 0 ~}
                            # C4D machine types require Hyperdisk (hyperdisk-balanced or hyperdisk-extreme)
                            installGpuDrivers: false
                            policy:
                              machineType: $${machineTypeC}
                              provisioningModel: STANDARD
                              bootDisk:
                                type: hyperdisk-balanced
                                sizeGb: 50
                            %{~ else ~}
                            policy:
                              machineType: $${machineTypeC}
                            %{~ endif ~}
                        %{~ else ~}
                        instances:
                          - policy: {}
                        %{~ endif ~}
                        labels:
                          component: epymodelingsuite
                          project: epymodelingsuite-cloud
                          environment: production
                          stage: output
                          managed-by: workflows
                    query:
                      jobId: $${"stage-c-" + uniqueId}
                  result: jobC

              - wait_for_stageC_creation:
                  call: sys.sleep
                  args:
                    seconds: 5

              - wait_for_stageC_completion:
                  call: waitJob
                  args:
                    name: $${jobC.body.name}

              - assign_stageC_job:
                  assign:
                    - stageCJobName: $${jobC.body.name}
          - condition: $${runOutputStage == false}
            steps:
              - log_output_skipped:
                  call: sys.log
                  args:
                    text: "Stage C (Output) skipped per configuration"
                    severity: INFO
              - assign_stageC_null:
                  assign:
                    - stageCJobName: null

    # ========== COMPLETION ==========
    - return_results:
        return:
          N: $${N}
          stageA_job: $${jobA.body.name}
          stageB_job: $${jobB.body.name}
          stageC_job: $${stageCJobName}

# ========== SUBWORKFLOW: Wait for Batch Job Completion ==========
waitJob:
  params: [name]
  steps:
    - init_wait:
        assign:
          - waitCount: 0
          - finalStatus: null

    - poll_loop:
        for:
          value: i
          range: $${[0, 2999]}
          steps:
            - get_job_status:
                try:
                  call: http.get
                  args:
                    url: $${"https://batch.googleapis.com/v1/" + name}
                    auth:
                      type: OAuth2
                  result: j
                except:
                  as: e
                  steps:
                    - log_error:
                        call: sys.log
                        args:
                          text: $${"Failed to get job status. Retrying in 10s."}
                          severity: WARNING
                    - sleep_on_error:
                        call: sys.sleep
                        args:
                          seconds: 10
                    - continue_loop:
                        next: continue

            - extract_state:
                assign:
                  - currentState: $${if("status" in j.body and "state" in j.body.status, j.body.status.state, "NO_STATUS")}
                  - waitCount: $${waitCount + 1}

            - log_status:
                call: sys.log
                args:
                  text: $${"Polling batch job status (iteration=" + string(waitCount) + "). State is " + currentState + "."}
                  severity: INFO

            - check_if_succeeded:
                switch:
                  - condition: $${currentState == "SUCCEEDED"}
                    steps:
                      - log_success:
                          call: sys.log
                          args:
                            text: $${"Batch job succeeded after " + string(waitCount) + " polling attempts"}
                            severity: INFO
                      - save_status:
                          assign:
                            - finalStatus: $${j.body.status}
                      - exit_loop:
                          next: break

            - check_if_failed:
                switch:
                  - condition: $${currentState in ["FAILED", "DELETION_IN_PROGRESS"]}
                    raise: '$${currentState}'

            - sleep_before_retry:
                call: sys.sleep
                args:
                  seconds: 15

    - return_status:
        return: $${finalStatus}

# ========== SUBWORKFLOW: Wait for Files in GCS ==========
waitForFiles:
  params: [bucket, prefix, maxRetries, sleepSeconds]
  steps:
    - init_file_wait:
        assign:
          - foundFiles: null

    - poll_for_files:
        for:
          value: attempt
          range: $${[1, maxRetries]}
          steps:
            - list_files:
                call: http.get
                args:
                  url: $${"https://storage.googleapis.com/storage/v1/b/" + bucket + "/o?prefix=" + prefix}
                  auth:
                    type: OAuth2
                result: ls

            - count_files:
                assign:
                  - items: $${default(map.get(ls.body, "items"), [])}
                  - fileCount: $${len(items)}

            - log_file_check:
                call: sys.log
                args:
                  text: $${"File check attempt " + string(attempt) + " of " + string(maxRetries) + ". Found " + string(fileCount) + " files."}
                  severity: INFO

            - check_files_found:
                switch:
                  - condition: $${fileCount > 0}
                    steps:
                      - log_files_found:
                          call: sys.log
                          args:
                            text: $${"Found " + string(fileCount) + " files at " + prefix}
                            severity: INFO
                      - save_files:
                          assign:
                            - foundFiles: $${items}
                      - exit_file_loop:
                          next: break

            - check_timeout:
                switch:
                  - condition: $${attempt >= maxRetries}
                    raise: "Timeout waiting for files"

            - sleep_before_retry:
                call: sys.sleep
                args:
                  seconds: $${sleepSeconds}

    - return_files:
        return: $${foundFiles}
