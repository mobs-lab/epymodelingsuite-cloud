"""Cloud job execution for run command."""

import json
import os
import subprocess
import tempfile
import uuid
from typing import Any

from epycloud.lib.command_helpers import (
    generate_run_id,
    get_batch_config,
    get_batch_service_account,
    get_github_config,
    get_image_uri,
    handle_dry_run,
)
from epycloud.lib.output import error, info, status, success, warning

from ..validation import (
    add_stage_specific_info,
    build_base_confirmation_info,
    prompt_user_confirmation,
    validate_and_get_machine_specs,
)
from .batch_config import build_batch_job_config


def run_job_cloud(
    ctx: dict[str, Any],
    config: dict[str, Any],
    stage: str,
    exp_id: str,
    run_id: str | None,
    task_index: int,
    num_tasks: int | None,
    output_config: str | None,
    machine_type_override: str | None,
    task_count_per_node: int | None,
    wait: bool,
    auto_confirm: bool,
    verbose: bool,
    dry_run: bool,
) -> int:
    """Submit individual job to Cloud Batch.

    Parameters
    ----------
    ctx : dict[str, Any]
        Command context
    config : dict[str, Any]
        Configuration dict
    stage : str
        Stage (A, B, or C)
    exp_id : str
        Experiment ID
    run_id : str | None
        Run ID
    task_index : int
        Task index for stage B
    num_tasks : int | None
        Number of tasks for stage C
    output_config : str | None
        Output config filename for Stage C (e.g., "output_projection.yaml")
    machine_type_override : str | None
        Override machine type for this job (auto-sets CPU/memory to machine max)
    task_count_per_node : int | None
        Max tasks per VM node (1 = dedicated VM per task)
    wait : bool
        Wait for completion
    auto_confirm : bool
        Auto-confirm without prompting
    verbose : bool
        Verbose output
    dry_run : bool
        Dry run mode

    Returns
    -------
    int
        Exit code
    """
    # Get configuration
    google_cloud = config.get("google_cloud", {})
    project_id = google_cloud.get("project_id")
    region = google_cloud.get("region", "us-central1")
    bucket_name = google_cloud.get("bucket_name")
    pipeline = config.get("pipeline", {})
    dir_prefix = pipeline.get("dir_prefix", "pipeline/flu/")
    batch_config = get_batch_config(config)
    github = get_github_config(config)
    github_forecast_repo = github["forecast_repo"]

    # Get image info
    image_uri = get_image_uri(config)

    # Get stage-specific resources
    stage_key = f"stage_{stage.lower()}"
    stage_config = batch_config.get(stage_key, {})
    cpu_milli = stage_config.get("cpu_milli", 2000)
    memory_mib = stage_config.get("memory_mib", 8192)
    machine_type = stage_config.get("machine_type", "")
    max_run_duration = stage_config.get("max_run_duration", 3600)

    # Apply machine type override if provided
    if machine_type_override:
        result = validate_and_get_machine_specs(
            machine_type_override, f"Stage {stage}", project_id, region
        )
        if result is None:
            return 1
        cpu_milli, memory_mib = result
        machine_type = machine_type_override

    # Set default for task_count_per_node if not provided
    if not task_count_per_node:
        task_count_per_node = batch_config.get("task_count_per_node", 1)

    # Validate
    if not project_id or not bucket_name:
        error("Missing required configuration: project_id or bucket_name")
        return 2

    # Get batch service account
    batch_sa_email = get_batch_service_account(project_id)

    # Generate job ID with 8-character UUID (consistent with workflow pattern)
    unique_id = str(uuid.uuid4())[:8]
    job_id = f"stage-{stage.lower()}-manual-{unique_id}"

    # Auto-generate run_id for stage A if not provided
    if stage == "A" and not run_id:
        run_id = generate_run_id()

    # Build confirmation info
    confirmation_info = build_base_confirmation_info(
        ctx, "job", exp_id, run_id if run_id else "<auto-generated>"
    )
    confirmation_info.update(
        {
            "project_id": project_id,
            "region": region,
            "stage": stage,
            "machine_type": machine_type,
            "max_duration_hours": max_run_duration // 3600,
            "image_uri": image_uri,
        }
    )
    add_stage_specific_info(confirmation_info, stage, task_index, num_tasks, output_config)

    confirmation_info["cpu_milli"] = cpu_milli
    confirmation_info["memory_mib"] = memory_mib
    confirmation_info["task_count_per_node"] = task_count_per_node

    # Show confirmation and prompt
    if not prompt_user_confirmation(auto_confirm, confirmation_info, mode="cloud"):
        return 0

    status(f"Submitting Stage {stage} job to Cloud Batch...")

    # Build job configuration
    job_config = build_batch_job_config(
        stage=stage,
        exp_id=exp_id,
        run_id=run_id,
        task_index=task_index,
        num_tasks=num_tasks,
        output_config=output_config,
        image_uri=image_uri,
        bucket_name=bucket_name,
        dir_prefix=dir_prefix,
        github_forecast_repo=github_forecast_repo,
        project_id=project_id,
        cpu_milli=cpu_milli,
        memory_mib=memory_mib,
        machine_type=machine_type,
        max_run_duration=max_run_duration,
        task_count_per_node=task_count_per_node,
        batch_sa_email=batch_sa_email,
    )

    if handle_dry_run(
        {"dry_run": dry_run},
        f"Submit batch job {job_id}",
        {"job_config": json.dumps(job_config, indent=2)},
    ):
        return 0

    # Write config to temp file
    with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
        json.dump(job_config, f, indent=2)
        temp_file = f.name

    try:
        # Submit job
        cmd = [
            "gcloud",
            "batch",
            "jobs",
            "submit",
            job_id,
            f"--project={project_id}",
            f"--location={region}",
            f"--config={temp_file}",
        ]

        result = subprocess.run(cmd, check=False)

        if result.returncode != 0:
            error("Job submission failed")
            return 1

        success("Job submitted successfully!")
        info(f"Job Name: projects/{project_id}/locations/{region}/jobs/{job_id}")
        print()
        info("Monitor with:")
        info(f"  gcloud batch jobs describe {job_id} --location={region}")
        print()
        info("View logs:")
        info(
            f'  gcloud logging read \'resource.type="batch.googleapis.com/Job" '
            f'AND labels.job_uid="{job_id}"\' --limit=50'
        )

        if wait:
            warning("--wait not yet implemented")
            info("Use: gcloud batch jobs describe --wait")

        return 0

    finally:
        # Clean up temp file
        try:
            os.unlink(temp_file)
        except OSError:
            pass
